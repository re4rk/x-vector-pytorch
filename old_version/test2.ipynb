{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: cvqluu\n",
    "repo: https://github.com/cvqluu/TDNN\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TDNN(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "                    self, \n",
    "                    input_dim=23, \n",
    "                    output_dim=512,\n",
    "                    context_size=5,\n",
    "                    stride=1,\n",
    "                    dilation=1,\n",
    "                    batch_norm=False,\n",
    "                    dropout_p=0.2\n",
    "                ):\n",
    "        '''\n",
    "        TDNN as defined by https://www.danielpovey.com/files/2015_interspeech_multisplice.pdf\n",
    "\n",
    "        Affine transformation not applied globally to all frames but smaller windows with local context\n",
    "\n",
    "        batch_norm: True to include batch normalisation after the non linearity\n",
    "        \n",
    "        Context size and dilation determine the frames selected\n",
    "        (although context size is not really defined in the traditional sense)\n",
    "        For example:\n",
    "            context size 5 and dilation 1 is equivalent to [-2,-1,0,1,2]\n",
    "            context size 3 and dilation 2 is equivalent to [-2, 0, 2]\n",
    "            context size 1 and dilation 1 is equivalent to [0]\n",
    "        '''\n",
    "        super(TDNN, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.stride = stride\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dilation = dilation\n",
    "        self.dropout_p = dropout_p\n",
    "        self.batch_norm = batch_norm\n",
    "      \n",
    "        self.kernel = nn.Linear(input_dim*context_size, output_dim)\n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        if self.batch_norm:\n",
    "            self.bn = nn.BatchNorm1d(output_dim)\n",
    "        if self.dropout_p:\n",
    "            self.drop = nn.Dropout(p=self.dropout_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        input: size (batch, seq_len, input_features)\n",
    "        outpu: size (batch, new_seq_len, output_features)\n",
    "        '''\n",
    "        \n",
    "        _, _, d = x.shape\n",
    "        assert (d == self.input_dim), 'Input dimension was wrong. Expected ({}), got ({})'.format(self.input_dim, d)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Unfold input into smaller temporal contexts\n",
    "        x = F.unfold(\n",
    "                        x, \n",
    "                        (self.context_size, self.input_dim), \n",
    "                        stride=(1,self.input_dim), \n",
    "                        dilation=(self.dilation,1)\n",
    "                    )\n",
    "\n",
    "        # N, output_dim*context_size, new_t = x.shape\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.kernel(x.float())\n",
    "        x = self.nonlinearity(x)\n",
    "        \n",
    "        if self.dropout_p:\n",
    "            x = self.drop(x)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            x = x.transpose(1,2)\n",
    "            x = self.bn(x)\n",
    "            x = x.transpose(1,2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdnn = TDNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4404, 0.2207, 0.8458,  ..., 0.4892, 0.4366, 0.1023],\n",
       "         [0.7678, 0.7187, 0.8508,  ..., 0.0429, 0.3646, 0.1251],\n",
       "         [0.5968, 0.8466, 0.3945,  ..., 0.7244, 0.5966, 0.8312],\n",
       "         ...,\n",
       "         [0.1986, 0.8062, 0.8846,  ..., 0.3881, 0.3388, 0.5566],\n",
       "         [0.4716, 0.9867, 0.6578,  ..., 0.0413, 0.8325, 0.4968],\n",
       "         [0.4873, 0.4935, 0.7286,  ..., 0.1536, 0.3410, 0.0715]],\n",
       "\n",
       "        [[0.1184, 0.2546, 0.9143,  ..., 0.6843, 0.5807, 0.1920],\n",
       "         [0.5552, 0.0059, 0.2573,  ..., 0.3597, 0.2361, 0.3734],\n",
       "         [0.2397, 0.4276, 0.0714,  ..., 0.1629, 0.0315, 0.3427],\n",
       "         ...,\n",
       "         [0.5957, 0.9486, 0.0627,  ..., 0.2708, 0.7658, 0.2871],\n",
       "         [0.3384, 0.7126, 0.5000,  ..., 0.0678, 0.3347, 0.1003],\n",
       "         [0.4883, 0.3739, 0.5029,  ..., 0.6770, 0.9791, 0.5804]],\n",
       "\n",
       "        [[0.7110, 0.7222, 0.8490,  ..., 0.3204, 0.4833, 0.6952],\n",
       "         [0.1114, 0.9234, 0.6911,  ..., 0.7575, 0.3362, 0.0637],\n",
       "         [0.9070, 0.0801, 0.9693,  ..., 0.4854, 0.7429, 0.7666],\n",
       "         ...,\n",
       "         [0.6285, 0.2868, 0.6965,  ..., 0.1756, 0.5021, 0.8686],\n",
       "         [0.3594, 0.2122, 0.5154,  ..., 0.0212, 0.0062, 0.7787],\n",
       "         [0.3297, 0.7414, 0.0375,  ..., 0.7022, 0.1280, 0.7359]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.9632, 0.6038, 0.4458,  ..., 0.7460, 0.9628, 0.6201],\n",
       "         [0.2640, 0.7361, 0.9368,  ..., 0.4427, 0.1667, 0.9579],\n",
       "         [0.3081, 0.1941, 0.7687,  ..., 0.8133, 0.2480, 0.6909],\n",
       "         ...,\n",
       "         [0.4876, 0.8676, 0.3666,  ..., 0.7271, 0.4702, 0.4468],\n",
       "         [0.3866, 0.3693, 0.9972,  ..., 0.5189, 0.7732, 0.5838],\n",
       "         [0.9315, 0.4267, 0.7455,  ..., 0.7161, 0.2286, 0.4782]],\n",
       "\n",
       "        [[0.5485, 0.6200, 0.0899,  ..., 0.5325, 0.5084, 0.8774],\n",
       "         [0.1931, 0.0500, 0.1305,  ..., 0.9159, 0.7012, 0.5561],\n",
       "         [0.8658, 0.4544, 0.3548,  ..., 0.9120, 0.5696, 0.8331],\n",
       "         ...,\n",
       "         [0.5501, 0.5374, 0.5290,  ..., 0.9361, 0.4435, 0.0257],\n",
       "         [0.6581, 0.2485, 0.7064,  ..., 0.3257, 0.6196, 0.8852],\n",
       "         [0.6563, 0.4376, 0.7617,  ..., 0.0161, 0.6147, 0.8782]],\n",
       "\n",
       "        [[0.9362, 0.2573, 0.8786,  ..., 0.3693, 0.2751, 0.2081],\n",
       "         [0.2451, 0.7464, 0.2926,  ..., 0.7523, 0.7574, 0.7515],\n",
       "         [0.2691, 0.9240, 0.9723,  ..., 0.9494, 0.2938, 0.6290],\n",
       "         ...,\n",
       "         [0.2276, 0.6481, 0.9933,  ..., 0.6122, 0.1720, 0.8049],\n",
       "         [0.0067, 0.4720, 0.3203,  ..., 0.7016, 0.6518, 0.3398],\n",
       "         [0.8808, 0.9109, 0.6356,  ..., 0.7766, 0.5845, 0.8818]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(10, 100, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0365,  ..., 0.8219, 0.3764, 0.1712],\n",
       "         [0.1364, 0.0000, 0.0247,  ..., 0.5945, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.7244, 0.3020, 0.1773],\n",
       "         ...,\n",
       "         [0.2313, 0.0000, 0.0000,  ..., 0.0000, 0.1881, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0536,  ..., 0.0000, 0.4088, 0.1005],\n",
       "         [0.2669, 0.0000, 0.3220,  ..., 0.5542, 0.1776, 0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdnn(torch.rand(1, 100, 23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDNN(\n",
      "  (kernel): Linear(in_features=115, out_features=512, bias=True)\n",
      "  (nonlinearity): ReLU()\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(tdnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bcb5cb247d89ced0608e588b2cce1c3da0f8550c83cac0a461e978202a0fb7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
